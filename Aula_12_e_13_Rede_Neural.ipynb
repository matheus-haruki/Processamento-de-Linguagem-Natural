{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcapNnYiaHZvkmQIMWCCC1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matheus-haruki/Processamento-de-Linguagem-Natural/blob/main/Aula_12_e_13_Rede_Neural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objetivos da aula**\n",
        "\n",
        "Introdução aos conceitos de Redes Neurais Recorrentes (RNNs) e Long Short-Term Memory Networks (LSTMs) e sua aplicação em tarefas de Processamento de Linguagem Natural\n",
        "\n",
        "**Técnicas de PLN implementadas:**\n",
        "\n",
        "\n",
        "· Implementar uma Rede Neural Recorrente (RNN) simples em Python para prever a próxima palavra em uma sequência de texto, utilizando a biblioteca TensorFlow/Keras.\n",
        "\n",
        "· Implementar uma Rede Long Short-Term Memory (LSTM) em Python para classificar o sentimento de frases como \"positivo\" ou \"negativo\", utilizando a biblioteca TensorFlow/Keras.\n",
        "\n",
        "**Passo a passo:**\n",
        "\n",
        "Implementação 1: Modelo de Rede Neural de Recorrência\n",
        "\n"
      ],
      "metadata": {
        "id": "Gt26lOZkV00u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Etapa 1: Configuração do ambiente no Google Colab\n",
        "\n",
        "#Importar bibliotecas necessárias\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "print(\"Bibliotecas importadas com sucesso!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqwAiMhtW31z",
        "outputId": "6b10863d-3582-4eec-98ff-866f955c3570"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Etapa 2: Preparação do Comjunto de Dados\n",
        "\n",
        "# Conjunto de dados de treinamento (pequeno e simplificado)\n",
        "textos_treinamento = [\n",
        "    \"eu gosto de programar em python\",\n",
        "    \"python é uma linguagem poderosa\",\n",
        "    \"programar é divertido com python\",\n",
        "    \"aprenda python e seja feliz\",\n",
        "    \"gosto de aprender coisas novas\"\n",
        "]\n",
        "\n",
        "print(f\"Textos de treinamento: {textos_treinamento}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtd6zUV4GbKc",
        "outputId": "138f5ef6-2277-4d81-a10e-144b6162ac78"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textos de treinamento: ['eu gosto de programar em python', 'python é uma linguagem poderosa', 'programar é divertido com python', 'aprenda python e seja feliz', 'gosto de aprender coisas novas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar o Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Construir o vocabulário a partir dos textos\n",
        "tokenizer.fit_on_texts(textos_treinamento)\n",
        "\n",
        "# Converter textos em sequências de números\n",
        "sequencias = tokenizer.texts_to_sequences(textos_treinamento)\n",
        "\n",
        "# Imprimir o vocabulário e as sequências geradas\n",
        "print(f\"\\nvocabulário (palavra: índice): {tokenizer.word_index}\")\n",
        "print(f\"Sequências numéricas dos textos: {sequencias}\")\n",
        "\n",
        "# Calcular o tamanho do vocabulário (+1 para incluir o 0 de padding)\n",
        "total_palavras = len(tokenizer.word_index) + 1\n",
        "print(f\"Tamanho total do vocabulário: {total_palavras}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e9HLf46JAYI",
        "outputId": "729ef830-20d8-4796-96b7-aa73940d9a02"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "vocabulário (palavra: índice): {'python': 1, 'gosto': 2, 'de': 3, 'programar': 4, 'é': 5, 'eu': 6, 'em': 7, 'uma': 8, 'linguagem': 9, 'poderosa': 10, 'divertido': 11, 'com': 12, 'aprenda': 13, 'e': 14, 'seja': 15, 'feliz': 16, 'aprender': 17, 'coisas': 18, 'novas': 19}\n",
            "Sequências numéricas dos textos: [[6, 2, 3, 4, 7, 1], [1, 5, 8, 9, 10], [4, 5, 11, 12, 1], [13, 1, 14, 15, 16], [2, 3, 17, 18, 19]]\n",
            "Tamanho total do vocabulário: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparar Entradas (X) e Saídas (y) para a previsão da próxima palavra\n",
        "# a entrada (X) será uma sequência de palavras, e a saída (y) será a palavra seguinte.\n",
        "# Determinar o comprimento máximo das sequências para padding\n",
        "max_comprimento = max(len(seq) for seq in sequencias)\n",
        "print(f\"\\nComprimento máximo das sequências antes do padding: {max_comprimento}\")\n",
        "\n",
        "# Criar pares de entrada (sequência parcial) e saída (próxima palavra)\n",
        "# Ex: \"eu gosto de programar\" -> \"em\"\n",
        "# \"gosto de programar em\" -> \"python\"\n",
        "entradas_X = []\n",
        "saídas_y = []\n",
        "\n",
        "for seq in sequencias:\n",
        "    for i in range(1, len(seq)):\n",
        "        entradas_X.append(seq[:i])  # A sequência até a palavra atual\n",
        "        saídas_y.append(seq[i])     # A próxima palavra\n",
        "\n",
        "print(f\"Exemplo de entradas_X (parcial): {entradas_X[0:5]}\")\n",
        "print(f\"Exemplo de saídas_y (parcial): {saídas_y[0:5]}\")\n",
        "\n",
        "# Padronizar o comprimento das sequências de entrada\n",
        "# Todas as sequências de entrada precisam ter o mesmo comprimento para a RNN\n",
        "entradas_X_padded = pad_sequences(entradas_X, maxlen=max_comprimento - 1, padding='pre')\n",
        "# O maxlen é 'max_comprimento - 1' porque a saída 'y' é a última palavra, então X sempre terá 1 palavra a menos.\n",
        "\n",
        "# Converter as saídas para o formato one-hot encoding\n",
        "# Isso é necessário para a camada de saída da RNN (softmax)\n",
        "saídas_y_one_hot = tf.keras.utils.to_categorical(saídas_y, num_classes=total_palavras)\n",
        "\n",
        "print(f\"\\nExemplo de entradas_X_padded (após padding e truncagem): \\n{entradas_X_padded[0:5]}\")\n",
        "print(f\"Exemplo de saídas_y_one_hot (após one-hot encoding): \\n{saídas_y_one_hot[0:5]}\")\n",
        "print(f\"Formato final das entradas (X): {entradas_X_padded.shape}\")\n",
        "print(f\"Formato final das saídas (y): {saídas_y_one_hot.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJw4oLTFMAa4",
        "outputId": "b41db11e-6184-4ea0-d0f2-50d0e4da2543"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comprimento máximo das sequências antes do padding: 6\n",
            "Exemplo de entradas_X (parcial): [[6], [6, 2], [6, 2, 3], [6, 2, 3, 4], [6, 2, 3, 4, 7]]\n",
            "Exemplo de saídas_y (parcial): [2, 3, 4, 7, 1]\n",
            "\n",
            "Exemplo de entradas_X_padded (após padding e truncagem): \n",
            "[[0 0 0 0 6]\n",
            " [0 0 0 6 2]\n",
            " [0 0 6 2 3]\n",
            " [0 6 2 3 4]\n",
            " [6 2 3 4 7]]\n",
            "Exemplo de saídas_y_one_hot (após one-hot encoding): \n",
            "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "Formato final das entradas (X): (21, 5)\n",
            "Formato final das saídas (y): (21, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Etapa 3: Construção do Modelo RNN\n",
        "\n",
        "# 1. Definindo o modelo\n",
        "# Definir a arquitetura do modelo RNN\n",
        "modelo_rnn = Sequential()\n",
        "\n",
        "# Camada de Embedding:\n",
        "# total_palavras: tamanho do vocabulário\n",
        "# 10: dimensão do vetor de embedding (quantas características queremos para cada palavra)\n",
        "# input_length: comprimento das sequências de entrada (maxlen - 1)\n",
        "modelo_rnn.add(Embedding(total_palavras, 10, input_length=entradas_X_padded.shape[1]))\n",
        "\n",
        "# Camada SimpleRNN:\n",
        "# 32: número de unidades (neurônios) na camada recorrente. Este é o tamanho do estado oculto.\n",
        "modelo_rnn.add(SimpleRNN(32))\n",
        "\n",
        "# Camada Densa de Saída:\n",
        "# total_palavras: número de neurônios de saída (um para cada palavra no vocabulário)\n",
        "# activation='softmax': função de ativação para probabilidade (soma 1 para todas as palavras)\n",
        "modelo_rnn.add(Dense(total_palavras, activation='softmax'))\n",
        "\n",
        "# Compilar o modelo\n",
        "modelo_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Exibir um resumo da arquitetura do modelo\n",
        "modelo_rnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "YEEQvfNRMFIM",
        "outputId": "a50dd3c6-cddf-430e-a09b-2b8fb4972672"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Etapa 4: Treinamento do Modelo\n",
        "\n",
        "# Treinando o modelo\n",
        "print(\"\\nIniciando o treinamento do modelo RNN...\")\n",
        "modelo_rnn.fit(entradas_X_padded, saídas_y_one_hot, epochs=100, verbose=1)\n",
        "   # epochs: quantas vezes o modelo verá todo o conjunto de dados\n",
        "   # verbose: 1 para mostrar o progresso do treinamento\n",
        "print(\"Treinamento concluído!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkCdQPWZNYaQ",
        "outputId": "c6d4baa7-4f54-4bca-8699-12648f261967"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando o treinamento do modelo RNN...\n",
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 2.9824\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.0476 - loss: 2.9748\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.0000e+00 - loss: 2.9673\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.0000e+00 - loss: 2.9596\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0476 - loss: 2.9518\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.0952 - loss: 2.9437\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1905 - loss: 2.9355\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2381 - loss: 2.9269\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1905 - loss: 2.9181\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1429 - loss: 2.9089\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.0952 - loss: 2.8993\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.0952 - loss: 2.8893\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1429 - loss: 2.8789\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1429 - loss: 2.8681\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1429 - loss: 2.8569\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1429 - loss: 2.8454\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.1429 - loss: 2.8334\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1429 - loss: 2.8212\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.0952 - loss: 2.8088\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.0952 - loss: 2.7962\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.0952 - loss: 2.7837\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.0952 - loss: 2.7712\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.0952 - loss: 2.7590\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.0952 - loss: 2.7471\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.0952 - loss: 2.7357\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.0952 - loss: 2.7246\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.0952 - loss: 2.7140\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0952 - loss: 2.7035\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.0952 - loss: 2.6932\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1429 - loss: 2.6827\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1429 - loss: 2.6720\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1429 - loss: 2.6609\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1429 - loss: 2.6494\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.1429 - loss: 2.6376\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1429 - loss: 2.6253\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1905 - loss: 2.6128\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1905 - loss: 2.6000\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.1905 - loss: 2.5869\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1905 - loss: 2.5736\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1905 - loss: 2.5600\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.2381 - loss: 2.5461\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2381 - loss: 2.5319\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3333 - loss: 2.5171\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3810 - loss: 2.5019\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.3810 - loss: 2.4861\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3810 - loss: 2.4698\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3810 - loss: 2.4528\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.3810 - loss: 2.4352\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4286 - loss: 2.4170\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.4286 - loss: 2.3981\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.4286 - loss: 2.3787\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4286 - loss: 2.3586\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.4286 - loss: 2.3380\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.4762 - loss: 2.3169\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.4762 - loss: 2.2952\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.5238 - loss: 2.2730\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.5238 - loss: 2.2503\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.5238 - loss: 2.2272\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.5238 - loss: 2.2035\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5238 - loss: 2.1795\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.5238 - loss: 2.1552\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5238 - loss: 2.1305\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.5714 - loss: 2.1056\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5714 - loss: 2.0805\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.5714 - loss: 2.0552\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.6190 - loss: 2.0299\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6190 - loss: 2.0044\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.6190 - loss: 1.9790\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.6190 - loss: 1.9535\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6190 - loss: 1.9280\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6190 - loss: 1.9026\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6190 - loss: 1.8773\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6190 - loss: 1.8521\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6667 - loss: 1.8270\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6667 - loss: 1.8021\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6667 - loss: 1.7773\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6667 - loss: 1.7528\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6667 - loss: 1.7284\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6667 - loss: 1.7042\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6667 - loss: 1.6802\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6667 - loss: 1.6563\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.6667 - loss: 1.6327\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7143 - loss: 1.6093\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7619 - loss: 1.5861\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7619 - loss: 1.5632\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7619 - loss: 1.5404\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7619 - loss: 1.5178\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7619 - loss: 1.4955\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7619 - loss: 1.4733\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7619 - loss: 1.4514\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7619 - loss: 1.4297\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7619 - loss: 1.4082\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7619 - loss: 1.3870\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7619 - loss: 1.3659\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7619 - loss: 1.3451\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8095 - loss: 1.3245\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8095 - loss: 1.3042\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8095 - loss: 1.2841\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8095 - loss: 1.2642\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8095 - loss: 1.2445\n",
            "Treinamento concluído!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Etapa 5: Usar o modelo para previsão\n",
        "\n",
        "# 1. Função de Previsão\n",
        "def prever_proxima_palavra(modelo, tokenizer, max_seq_len, texto_base):\n",
        "    \"\"\"\n",
        "    Prevê a próxima palavra dado um texto base.\n",
        "    \"\"\"\n",
        "    # Converter o texto base para sequência numérica\n",
        "    sequencia_numerica = tokenizer.texts_to_sequences([texto_base])[0]\n",
        "\n",
        "    # Padronizar o comprimento da sequência de entrada (como no treinamento)\n",
        "    sequencia_padded = pad_sequences([sequencia_numerica], maxlen=max_seq_len, padding='pre')\n",
        "\n",
        "    # Fazer a previsão\n",
        "    previsao_probabilidades = modelo.predict(sequencia_padded, verbose=0)[0]\n",
        "\n",
        "    # Obter o índice da palavra com maior probabilidade\n",
        "    indice_palavra_prevista = np.argmax(previsao_probabilidades)\n",
        "\n",
        "    # Converter o índice de volta para a palavra\n",
        "    for palavra, indice in tokenizer.word_index.items():\n",
        "        if indice == indice_palavra_prevista:\n",
        "            return palavra\n",
        "    return None  # Caso a palavra não seja encontrada\n",
        "\n",
        "# Comprimento de entrada esperado pelo modelo\n",
        "comprimento_entrada_modelo = entradas_X_padded.shape[1]\n",
        "\n",
        "# Testar o modelo com algumas frases\n",
        "print(\"\\n-- Testando o Modelo RNN --\")\n",
        "\n",
        "texto_teste_1 = \"eu gosto de\"\n",
        "proxima_1 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_1)\n",
        "print(f\"Texto: '{texto_teste_1}' -> Próxima palavra prevista: '{proxima_1}'\")\n",
        "\n",
        "texto_teste_2 = \"python é uma\"\n",
        "proxima_2 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_2)\n",
        "print(f\"Texto: '{texto_teste_2}' -> Próxima palavra prevista: '{proxima_2}'\")\n",
        "\n",
        "texto_teste_3 = \"programar é divertido\"\n",
        "proxima_3 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_3)\n",
        "print(f\"Texto: '{texto_teste_3}' -> Próxima palavra prevista: '{proxima_3}'\")\n",
        "\n",
        "texto_teste_4 = \"aprenda python e\"\n",
        "proxima_4 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_4)\n",
        "print(f\"Texto: '{texto_teste_4}' -> Próxima palavra prevista: '{proxima_4}'\")\n",
        "\n",
        "# Exemplo com palavras fora do vocabulário\n",
        "texto_teste_5 = \"o sol brilha no\"  # Palavras \"sol\" e \"brilha\" podem não estar no vocabulário\n",
        "proxima_5 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_5)\n",
        "print(f\"Texto: '{texto_teste_5}' -> Próxima palavra prevista: '{proxima_5}' (pode ser inesperada devido a palavras desconhecidas)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTZK5XKCOX9H",
        "outputId": "0377da85-eba9-4823-949c-0c20c3a5f649"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Testando o Modelo RNN --\n",
            "Texto: 'eu gosto de' -> Próxima palavra prevista: 'programar'\n",
            "Texto: 'python é uma' -> Próxima palavra prevista: 'linguagem'\n",
            "Texto: 'programar é divertido' -> Próxima palavra prevista: 'com'\n",
            "Texto: 'aprenda python e' -> Próxima palavra prevista: 'seja'\n",
            "Texto: 'o sol brilha no' -> Próxima palavra prevista: 'é' (pode ser inesperada devido a palavras desconhecidas)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementação 2: Modelo de Rede Neural Rede Long Short-Term Memory**"
      ],
      "metadata": {
        "id": "vMTmt9J6P8tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Etapa 1: Configuração do Ambiente e Importação de Bibliotecas\n",
        "\n",
        "# Important bibliotecas necessárias\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIcoF-8vNeya",
        "outputId": "cd98bfcf-e514-4820-d4d2-50d3946121ce"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Etapa 2: Preparação do conjunto de dados e análise de sentimentos\n",
        "\n",
        "# Definir o Conjunto de Dados (Frases e Rótulos) para análise de sentimentos\n",
        "dados_sentimento = [\n",
        " (\"este filme é ótimo e divertido\", \"positivo\"),\n",
        " (\"eu adorei o livro, muito bom\", \"positivo\"),\n",
        " (\"gostei muito da atuação dos atores\", \"positivo\"),\n",
        " (\"o roteiro é fraco e chato\", \"negativo\"),\n",
        " (\"não recomendo este péssimo produto\", \"negativo\"),\n",
        " (\"uma perda de tempo horrível\", \"negativo\"),\n",
        " (\"ótimo trabalho, parabéns\", \"positivo\"),\n",
        " (\"terrível experiência, nunca mais\", \"negativo\"),\n",
        " (\"excelente serviço, muito eficiente\", \"positivo\"),\n",
        " (\"que decepção, muito ruim\", \"negativo\"),\n",
        " (\"aprendizagem de máquina é fascinante\", \"positivo\"),\n",
        " (\"pln é um campo interessante\", \"positivo\"),\n",
        " (\"este software travou várias vezes\", \"negativo\"),\n",
        " (\"a interface é confusa e difícil\", \"negativo\"),\n",
        " (\"o aplicativo é super útil e rápido\", \"positivo\"),\n",
        "]\n",
        "\n",
        "textos = [dado[0] for dado in dados_sentimento]\n",
        "sentimentos = [dado[1] for dado in dados_sentimento]\n",
        "\n",
        "print(f\"Total de frases: {len(textos)}\")\n",
        "print(f\"Exemplo de textos: {textos[:3]}\")\n",
        "print(f\"Exemplo de sentimentos: {sentimentos[:3]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVHdbBwtQgfv",
        "outputId": "346bc049-7b81-4b50-e659-fe945f54556f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de frases: 15\n",
            "Exemplo de textos: ['este filme é ótimo e divertido', 'eu adorei o livro, muito bom', 'gostei muito da atuação dos atores']\n",
            "Exemplo de sentimentos: ['positivo', 'positivo', 'positivo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapear Sentimentos para Números: converter “positivo” e “negativo” para 0 e 1\n",
        "mapeamento_sentimento = {'negativo': 0, 'positivo': 1}\n",
        "\n",
        "rotulos_numericos = np.array([mapeamento_sentimento[s] for s in sentimentos])\n",
        "\n",
        "print(f\"\\nSentimentos mapeados para números: {rotulos_numericos}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMs7nI0xQ0U4",
        "outputId": "e20b230d-d678-4b69-d8df-5fd6d9c14a70"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentimentos mapeados para números: [1 1 1 0 0 0 1 0 1 0 1 1 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenização de Texto\n",
        "tokenizer = Tokenizer(num_words=None, oov_token=\"\")  # oov_token define o token para palavras desconhecidas\n",
        "tokenizer.fit_on_texts(textos)\n",
        "sequencias_numericas = tokenizer.texts_to_sequences(textos)\n",
        "\n",
        "total_palavras_vocab = len(tokenizer.word_index) + 1  # +1 para o índice 0 (usado no padding e OOV)\n",
        "\n",
        "print(f\"\\nVocabulário (palavra: índice): {tokenizer.word_index}\")\n",
        "print(f\"Sequências numéricas das frases: {sequencias_numericas}\")\n",
        "print(f\"Tamanho total do vocabulário: {total_palavras_vocab}\")\n",
        "\n",
        "# Padronizar o comprimento das sequências\n",
        "max_len = max(len(s) for s in sequencias_numericas)\n",
        "print(f\"\\nComprimento máximo das sequências: {max_len}\")\n",
        "\n",
        "sequencias_padded = pad_sequences(sequencias_numericas, maxlen=max_len, padding='post')\n",
        "print(f\"Sequências após padding: \\n{sequencias_padded}\")\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_treino, X_teste, y_treino, y_teste = train_test_split(\n",
        "    sequencias_padded,\n",
        "    rotulos_numericos,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=rotulos_numericos\n",
        ")\n",
        "\n",
        "print(f\"\\nShape de X_treino: {X_treino.shape}\")\n",
        "print(f\"Shape de X_teste: {X_teste.shape}\")\n",
        "print(f\"Shape de y_treino: {y_treino.shape}\")\n",
        "print(f\"Shape de y_teste: {y_teste.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmD-tzFbRbRJ",
        "outputId": "c0bb7fcd-540e-4a54-9951-ff01a3a91417"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulário (palavra: índice): {'': 1, 'é': 2, 'e': 3, 'muito': 4, 'este': 5, 'o': 6, 'ótimo': 7, 'de': 8, 'filme': 9, 'divertido': 10, 'eu': 11, 'adorei': 12, 'livro': 13, 'bom': 14, 'gostei': 15, 'da': 16, 'atuação': 17, 'dos': 18, 'atores': 19, 'roteiro': 20, 'fraco': 21, 'chato': 22, 'não': 23, 'recomendo': 24, 'péssimo': 25, 'produto': 26, 'uma': 27, 'perda': 28, 'tempo': 29, 'horrível': 30, 'trabalho': 31, 'parabéns': 32, 'terrível': 33, 'experiência': 34, 'nunca': 35, 'mais': 36, 'excelente': 37, 'serviço': 38, 'eficiente': 39, 'que': 40, 'decepção': 41, 'ruim': 42, 'aprendizagem': 43, 'máquina': 44, 'fascinante': 45, 'pln': 46, 'um': 47, 'campo': 48, 'interessante': 49, 'software': 50, 'travou': 51, 'várias': 52, 'vezes': 53, 'a': 54, 'interface': 55, 'confusa': 56, 'difícil': 57, 'aplicativo': 58, 'super': 59, 'útil': 60, 'rápido': 61}\n",
            "Sequências numéricas das frases: [[5, 9, 2, 7, 3, 10], [11, 12, 6, 13, 4, 14], [15, 4, 16, 17, 18, 19], [6, 20, 2, 21, 3, 22], [23, 24, 5, 25, 26], [27, 28, 8, 29, 30], [7, 31, 32], [33, 34, 35, 36], [37, 38, 4, 39], [40, 41, 4, 42], [43, 8, 44, 2, 45], [46, 2, 47, 48, 49], [5, 50, 51, 52, 53], [54, 55, 2, 56, 3, 57], [6, 58, 2, 59, 60, 3, 61]]\n",
            "Tamanho total do vocabulário: 62\n",
            "\n",
            "Comprimento máximo das sequências: 7\n",
            "Sequências após padding: \n",
            "[[ 5  9  2  7  3 10  0]\n",
            " [11 12  6 13  4 14  0]\n",
            " [15  4 16 17 18 19  0]\n",
            " [ 6 20  2 21  3 22  0]\n",
            " [23 24  5 25 26  0  0]\n",
            " [27 28  8 29 30  0  0]\n",
            " [ 7 31 32  0  0  0  0]\n",
            " [33 34 35 36  0  0  0]\n",
            " [37 38  4 39  0  0  0]\n",
            " [40 41  4 42  0  0  0]\n",
            " [43  8 44  2 45  0  0]\n",
            " [46  2 47 48 49  0  0]\n",
            " [ 5 50 51 52 53  0  0]\n",
            " [54 55  2 56  3 57  0]\n",
            " [ 6 58  2 59 60  3 61]]\n",
            "\n",
            "Shape de X_treino: (12, 7)\n",
            "Shape de X_teste: (3, 7)\n",
            "Shape de y_treino: (12,)\n",
            "Shape de y_teste: (3,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Etapa 3: Construção do Modelo LSTM\n",
        "\n",
        "#Definir a arquitetura do modelo LSTM\n",
        "modelo_lstm = Sequential()\n",
        "\n",
        "# Camada de Embedding: Converte os índices numéricos das palavras em vetores densos.\n",
        "# total_palavras_vocab: tamanho do vocabulário\n",
        "# 50: dimensão do vetor de embedding (pode ser ajustado)\n",
        "# input_length: comprimento padronizado das sequências (max_len)\n",
        "modelo_lstm.add(Embedding(total_palavras_vocab, 50, input_length=max_len))\n",
        "\n",
        "# Camada LSTM:\n",
        "# 64: número de unidades (neurônios) na camada LSTM. Define o tamanho do estado oculto e da célula de memória.\n",
        "# dropout: Um tipo de regularização para evitar overfitting (descarta aleatoriamente neurônios durante o treinamento).\n",
        "# recurrent_dropout: Dropout aplicado nas conexões recorrentes da LSTM.\n",
        "modelo_lstm.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "# Camada Densa de Saída:\n",
        "# 1: Um único neurônio de saída, pois é um problema de classificação binária (positivo/negativo).\n",
        "# activation='sigmoid': Função de ativação para classificação binária (produz um valor entre 0 e 1).\n",
        "modelo_lstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compilar o modelo\n",
        "modelo_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Exibir um resumo da arquitetura do modelo\n",
        "modelo_lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "y7U_WvhcRpMQ",
        "outputId": "3878ba59-46d6-4c18-bdfd-8ceb813387f2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Etapa 4: Treinamento e avaliação do Modelo\n",
        "\n",
        "# Treinar o modelo\n",
        "print(\"\\nIniciando o treinamento do modelo LSTM...\")\n",
        "historico = modelo_lstm.fit(\n",
        "    X_treino, y_treino,\n",
        "    epochs=50, # Reduzi para 50 epochs para um treinamento mais rápido no exemplo. Pode ser aumentado.\n",
        "    batch_size=2, # Pequeno batch_size para dataset pequeno.\n",
        "    validation_split=0.1, # Usar 10% do treino para validação\n",
        "    verbose=1\n",
        ")\n",
        "# epochs: número de vezes que o modelo verá todo o conjunto de dados de treinamento.\n",
        "# batch_size: número de amostras por atualização de gradiente.\n",
        "# validation_split: % dos dados de treino usados para validação durante o treinamento (opcional, mas bom para monitorar overfitting).\n",
        "print(\"treinamento concluído!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a93uMGUdSEzK",
        "outputId": "1977e556-b7ad-4182-8f20-dd2c2b550210"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando o treinamento do modelo LSTM...\n",
            "Epoch 1/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 191ms/step - accuracy: 0.4125 - loss: 0.6949 - val_accuracy: 0.0000e+00 - val_loss: 0.6972\n",
            "Epoch 2/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5861 - loss: 0.6900 - val_accuracy: 0.0000e+00 - val_loss: 0.6981\n",
            "Epoch 3/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7931 - loss: 0.6869 - val_accuracy: 0.0000e+00 - val_loss: 0.6986\n",
            "Epoch 4/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8764 - loss: 0.6815 - val_accuracy: 0.0000e+00 - val_loss: 0.6996\n",
            "Epoch 5/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.6739 - val_accuracy: 0.0000e+00 - val_loss: 0.7016\n",
            "Epoch 6/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.6616 - val_accuracy: 0.0000e+00 - val_loss: 0.7078\n",
            "Epoch 7/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.6419 - val_accuracy: 0.0000e+00 - val_loss: 0.7164\n",
            "Epoch 8/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.5935 - val_accuracy: 0.0000e+00 - val_loss: 0.7369\n",
            "Epoch 9/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.5344 - val_accuracy: 0.0000e+00 - val_loss: 0.7779\n",
            "Epoch 10/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.4023 - val_accuracy: 0.0000e+00 - val_loss: 0.8767\n",
            "Epoch 11/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.2753 - val_accuracy: 0.0000e+00 - val_loss: 1.1099\n",
            "Epoch 12/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.1172 - val_accuracy: 0.0000e+00 - val_loss: 1.6005\n",
            "Epoch 13/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0520 - val_accuracy: 0.0000e+00 - val_loss: 2.3147\n",
            "Epoch 14/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0226 - val_accuracy: 0.0000e+00 - val_loss: 3.0224\n",
            "Epoch 15/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0150 - val_accuracy: 0.0000e+00 - val_loss: 3.6024\n",
            "Epoch 16/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.0000e+00 - val_loss: 4.0080\n",
            "Epoch 17/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0068 - val_accuracy: 0.0000e+00 - val_loss: 4.3005\n",
            "Epoch 18/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.0000e+00 - val_loss: 4.4981\n",
            "Epoch 19/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.0000e+00 - val_loss: 4.6369\n",
            "Epoch 20/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.0000e+00 - val_loss: 4.7484\n",
            "Epoch 21/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.0000e+00 - val_loss: 4.8385\n",
            "Epoch 22/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.0000e+00 - val_loss: 4.9113\n",
            "Epoch 23/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.0000e+00 - val_loss: 4.9718\n",
            "Epoch 24/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.0000e+00 - val_loss: 5.0275\n",
            "Epoch 25/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.0000e+00 - val_loss: 5.0786\n",
            "Epoch 26/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.0000e+00 - val_loss: 5.1238\n",
            "Epoch 27/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.0000e+00 - val_loss: 5.1679\n",
            "Epoch 28/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.0000e+00 - val_loss: 5.2083\n",
            "Epoch 29/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.0000e+00 - val_loss: 5.2520\n",
            "Epoch 30/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.0000e+00 - val_loss: 5.2949\n",
            "Epoch 31/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.0000e+00 - val_loss: 5.3375\n",
            "Epoch 32/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 9.9078e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.3766\n",
            "Epoch 33/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.0000e+00 - val_loss: 5.4143\n",
            "Epoch 34/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.0000e+00 - val_loss: 5.4552\n",
            "Epoch 35/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 7.8742e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.4982\n",
            "Epoch 36/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 7.4471e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.5412\n",
            "Epoch 37/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.0000e+00 - val_loss: 5.5830\n",
            "Epoch 38/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 7.0057e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.6222\n",
            "Epoch 39/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 7.2825e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.6608\n",
            "Epoch 40/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 6.4514e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.6947\n",
            "Epoch 41/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 6.1341e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.7297\n",
            "Epoch 42/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 6.5535e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.7627\n",
            "Epoch 43/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 6.1799e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.7979\n",
            "Epoch 44/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 6.1638e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.8335\n",
            "Epoch 45/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 7.0667e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.8698\n",
            "Epoch 46/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 6.1358e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.9056\n",
            "Epoch 47/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.5991e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.9380\n",
            "Epoch 48/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 4.9962e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.9688\n",
            "Epoch 49/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 3.6266e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.9987\n",
            "Epoch 50/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 3.1786e-04 - val_accuracy: 0.0000e+00 - val_loss: 6.0247\n",
            "treinamento concluído!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Avalia o modelo no conjunto de teste\n",
        "perda, acuracia = modelo_lstm.evaluate(X_teste, y_teste, verbose=0)\n",
        "print(f\"\\nAcurácia do modelo no conjunto de teste: {acuracia * 100:.2f}%\")\n",
        "print(f\"Perda do modelo no conjunto de teste: {perda:.4f}\")\n",
        "\n",
        "# Fazer previsões no conjunto de teste\n",
        "y_pred_prob = modelo_lstm.predict(X_teste)\n",
        "y_pred_classes = (y_pred_prob > 0.5).astype(int)  # Converter probabilidades para 0 ou 1\n",
        "\n",
        "print(\"\\n--- Relatório de Classificação ---\")\n",
        "print(classification_report(y_teste, y_pred_classes, target_names=['negativo', 'positivo']))\n",
        "\n",
        "print(\"\\n--- Matriz de Confusão ---\")\n",
        "cm = confusion_matrix(y_teste, y_pred_classes)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['negativo', 'positivo'],\n",
        "            yticklabels=['negativo', 'positivo'])\n",
        "plt.xlabel('Previsto')\n",
        "plt.ylabel('Real')\n",
        "plt.title('Matriz de Confusão')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        },
        "id": "xLSm2GY2TL56",
        "outputId": "c52e526f-de21-479d-d37e-41460eb42ca9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acurácia do modelo no conjunto de teste: 66.67%\n",
            "Perda do modelo no conjunto de teste: 0.9828\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\n",
            "--- Relatório de Classificação ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo       0.50      1.00      0.67         1\n",
            "    positivo       1.00      0.50      0.67         2\n",
            "\n",
            "    accuracy                           0.67         3\n",
            "   macro avg       0.75      0.75      0.67         3\n",
            "weighted avg       0.83      0.67      0.67         3\n",
            "\n",
            "\n",
            "--- Matriz de Confusão ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Etapa 5: Testar o modelo com novas frases\n",
        "\n",
        "# Utilizando o modelo treinado\n",
        "def prever_sentimento(modelo, tokenizer, max_seq_len, frase_nova, mapeamento_sentimento):\n",
        "    \"\"\"\n",
        "    Prevê o sentimento de uma nova frase.\n",
        "    \"\"\"\n",
        "    # Converter a frase para sequência numérica\n",
        "    sequencia_numerica = tokenizer.texts_to_sequences([frase_nova])\n",
        "\n",
        "    # Verificar se a frase tem apenas palavras desconhecidas\n",
        "    if not sequencia_numerica or not sequencia_numerica[0]:\n",
        "        print(f\"Aviso: A frase '{frase_nova}' contém apenas palavras desconhecidas.\")\n",
        "        return \"Desconhecido\"\n",
        "\n",
        "    sequencia_numerica = sequencia_numerica[0]  # Pega a primeira (e única) sequência\n",
        "\n",
        "    # Padronizar o comprimento da sequência de entrada\n",
        "    sequencia_padded = pad_sequences([sequencia_numerica], maxlen=max_seq_len, padding='post')\n",
        "\n",
        "    # Fazer a previsão (probabilidade)\n",
        "    probabilidade_positiva = modelo.predict(sequencia_padded, verbose=0)[0][0]\n",
        "\n",
        "    # Inverter o mapeamento para obter o nome do sentimento\n",
        "    mapeamento_inverso = {v: k for k, v in mapeamento_sentimento.items()}\n",
        "\n",
        "    # Classificar com base no limiar de 0.5\n",
        "    if probabilidade_positiva >= 0.5:\n",
        "        return mapeamento_inverso[1]  # 'positivo'\n",
        "    else:\n",
        "        return mapeamento_inverso[0]  # 'negativo'\n",
        "\n",
        "\n",
        "# Testar o modelo com novas frases\n",
        "print(\"\\n--- Testando o Modelo LSTM com Novas Frases ---\")\n",
        "\n",
        "frases = [\n",
        "    \"gostei muito do filme, excelente!\",\n",
        "    \"odiei o livro, muito entediante\",\n",
        "    \"a aula de pln é ótima\",\n",
        "    \"o atendimento foi péssimo\",\n",
        "    \"esse produto não vale a pena, é caro\",\n",
        "    \"o filme é legal\",\n",
        "    \"isso é horrível, que tristeza\"\n",
        "]\n",
        "\n",
        "for i, frase in enumerate(frases, 1):\n",
        "    sentimento = prever_sentimento(modelo_lstm, tokenizer, max_len, frase, mapeamento_sentimento)\n",
        "    print(f\"Frase {i}: '{frase}' -> Sentimento previsto: '{sentimento}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLPR4pKjTYF4",
        "outputId": "8efb75f2-76b5-49fb-f2c4-d69579c4a074"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testando o Modelo LSTM com Novas Frases ---\n",
            "Frase 1: 'gostei muito do filme, excelente!' -> Sentimento previsto: 'positivo'\n",
            "Frase 2: 'odiei o livro, muito entediante' -> Sentimento previsto: 'positivo'\n",
            "Frase 3: 'a aula de pln é ótima' -> Sentimento previsto: 'positivo'\n",
            "Frase 4: 'o atendimento foi péssimo' -> Sentimento previsto: 'negativo'\n",
            "Frase 5: 'esse produto não vale a pena, é caro' -> Sentimento previsto: 'negativo'\n",
            "Frase 6: 'o filme é legal' -> Sentimento previsto: 'negativo'\n",
            "Frase 7: 'isso é horrível, que tristeza' -> Sentimento previsto: 'negativo'\n"
          ]
        }
      ]
    }
  ]
}